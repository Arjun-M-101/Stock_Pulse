# ğŸ“ˆ StockPulse â€“ Realâ€‘Time Data Engineering Pipeline (Streaming)

## ğŸš€ Overview
**StockPulse** is a **realâ€‘time data engineering pipeline** that simulates live stock index ticks and processes them endâ€‘toâ€‘end.  

It demonstrates a **modern streaming architecture** using:

- **Kafka** â†’ Message broker for realâ€‘time ingestion  
- **Apache Spark Structured Streaming** â†’ Scalable stream processing & ETL  
- **Postgres** â†’ Serving layer for analytics  
- **Parquet Data Lake** â†’ Partitioned storage for historical queries  
- **Apache Airflow (3.x)** â†’ Orchestration & scheduling  
- **Streamlit + Altair** â†’ Interactive BI dashboard  

The pipeline ingests simulated stock index data, enriches it with derived metrics, stores it in both **Postgres** and **Parquet**, and visualizes it in realâ€‘time.

---

## ğŸ—ï¸ Architecture

```
[ indexProcessed.csv ]
        â”‚
        â–¼
 [ Kafka Producer ]  --->  [ Kafka Topic: stock_ticks ]  --->  [ Spark Structured Streaming Consumer ]
                                                                 â”‚
                                                                 â”œâ”€â”€> [ Parquet Data Lake (partitioned by index/date) ]
                                                                 â”‚
                                                                 â””â”€â”€> [ Postgres DB (ticks_raw table) ]
                                                                                   â”‚
                                                                                   â–¼
                                                                       [ Streamlit Dashboard ]
                                                                                   â”‚
                                                                                   â–¼
                                                                         Interactive Visuals
                                                                                   â”‚
                                                                                   â–¼
                                                                         [ Airflow DAG Orchestration ]
```

---

## ğŸ“‚ Project Structure

```text
stock_pulse/
â”‚
â”œâ”€â”€ airflow/                      # Airflow home (local)
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ stock_pipeline_dag.py # Airflow DAG definition
â”‚   â”œâ”€â”€ logs/                     # Airflow logs
â”‚   â”œâ”€â”€ airflow.cfg               # Airflow config
â”‚   â””â”€â”€ airflow.db                # Airflow metadata DB
â”‚
â”œâ”€â”€ chk/                          # Spark checkpoints
â”‚   â”œâ”€â”€ parquet/
â”‚   â””â”€â”€ postgres/
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer.py               # Spark Structured Streaming consumer
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ indexProcessed.csv        # Input dataset for producer
â”‚
â”œâ”€â”€ lake/
â”‚   â””â”€â”€ parquet/                  # Partitioned parquet sink (index/date)
â”‚
â”œâ”€â”€ postgres/
â”‚   â””â”€â”€ init.sql                  # DB init script (optional)
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ producer.py               # Kafka producer (simulated ticks)
â”‚
â”œâ”€â”€ clear_outputs.py              # Utility to clear outputs/checkpoints
â”œâ”€â”€ requirements-lock.txt         # Pinned dependencies
â”œâ”€â”€ streamlit_app.py              # Streamlit + Altair dashboard
â””â”€â”€ venv/                         # Python virtual environment (local)
```

---

## ğŸ› ï¸ Prerequisites & Installation

### System Packages

- **Python 3.10+**
  ```bash
  sudo apt update
  sudo apt install -y python3 python3-venv python3-dev
  ```

- **Java (JDK 11+)** â†’ required for PySpark
  ```bash
  sudo apt install -y openjdk-11-jdk
  java -version
  ```

- **Apache Spark**
  ```bash
  wget https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
  tar xvf spark-3.4.1-bin-hadoop3.tgz
  mv spark-3.4.1-bin-hadoop3 ~/spark
  export SPARK_HOME=~/spark
  export PATH=$SPARK_HOME/bin:$PATH
  spark-shell --version
  ```

- **Kafka**
  ```bash
  wget https://downloads.apache.org/kafka/3.6.0/kafka_2.13-3.6.0.tgz
  tar -xzf kafka_2.13-3.6.0.tgz
  mv kafka_2.13-3.6.0 ~/kafka
  ```

  Start services in separate terminals:
  ```bash
  ~/kafka/bin/zookeeper-server-start.sh ~/kafka/config/zookeeper.properties
  ~/kafka/bin/kafka-server-start.sh ~/kafka/config/server.properties
  ```

- **Postgres**
  ```bash
  sudo apt install -y postgresql postgresql-contrib libpq-dev
  sudo systemctl start postgresql
  ```

  Create DB:
  ```sql
  CREATE DATABASE stocks;
  \c stocks
  ```

  Create Table:
  ```sql
  -- create fresh table
  CREATE TABLE ticks_raw (
    stream_ts TIMESTAMPTZ NOT NULL,
    date DATE NOT NULL,
    index TEXT NOT NULL,
    open NUMERIC,
    high NUMERIC,
    low NUMERIC,
    close NUMERIC NOT NULL,
    adj_close NUMERIC,
    volume BIGINT NOT NULL,
    close_usd NUMERIC,
    trade_value NUMERIC
  );
  ```
  
- **Airflow (3.x)**
  ```bash
  pip install apache-airflow
  airflow db migrate
  airflow standalone
  ```

---

## âš™ï¸ Setup Instructions

### Clone repo
```bash
git clone https://github.com/Arjun-M-101/StockPulse.git
cd stock_pulse
```

### Create virtual environment
```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements-lock.txt
```

### Start Kafka topic
```bash
~/kafka/bin/kafka-topics.sh --create --topic stock_ticks --bootstrap-server localhost:9092
```

### Run Producer
```bash
python producer/producer.py
```

### Run Consumer (Spark)
```bash
$SPARK_HOME/bin/spark-submit \
  --master local[*] \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.postgresql:postgresql:42.7.4 \
  consumer/consumer.py
```

### Trigger Airflow DAG
In Airflow UI â†’ enable and trigger `stockpulse_pipeline`.

### Launch Dashboard
```bash
streamlit run streamlit_app.py
```

### Reset Outputs (optional)
```bash
python clear_outputs.py
```

- Clears: `lake/parquet` contents and Spark checkpoint directories (`chk/parquet`, `chk/postgres`)  
- Restores: `.gitkeep` to keep folders tracked by Git  

---

## ğŸ“Š Dashboard Features

- **Raw Close Prices** â†’ Line chart of absolute price levels  
- **% Change** â†’ Relative moves per index  
- **Rebased to 100** â†’ Growth from baseline  
- **Volume Share** â†’ Market share by traded volume  

Autoâ€‘refreshes every 5 seconds for live updates.

---

## ğŸ”„ Data Flow

### **Producer**
- Reads `indexProcessed.csv`  
- Emits **1 row per index per tick** with synchronized timestamp  
- Publishes to Kafka topic `stock_ticks`

### **Consumer (Spark)**
- Reads from Kafka in streaming mode  
- Parses JSON â†’ applies schema  
- Adds derived column: `trade_value = close * volume`  
- Writes to:
  - **Parquet** (partitioned by index/date)  
  - **Postgres** (`ticks_raw` table)

### **Airflow DAG**
- Orchestrates Spark consumer job  
- Can be extended for retries, alerts, and scheduling

### **Dashboard**
- Connects to Postgres  
- Queries latest 500 ticks  
- Provides interactive Altair visualizations

### **Clear Outputs Utility**
- `clear_outputs.py` wipes **lake/parquet** and **chk/** directories  
- Restores `.gitkeep` to preserve folder structure  
- Useful for reproducibility and fresh pipeline runs

---

## âœ… Key Takeaways

- Demonstrates **realâ€‘time streaming pipeline** with Kafka + Spark  
- Dual sink: **Parquet (historical)** + **Postgres (serving)**  
- **Airflow DAG** for orchestration  
- **Streamlit dashboard** for recruiterâ€‘friendly visualization  
- **Clear outputs utility** for reproducible resets  
- Fully reproducible locally, but cloudâ€‘ready (S3, EMR, MWAA, RDS/Redshift)

---

## âš–ï¸ Tradeâ€‘offs & Design Decisions

- **Kafka vs. File Watcher** â†’ Kafka chosen for scalability & replayability  
- **Postgres vs. Data Warehouse** â†’ Postgres is lightweight for local dev; Redshift/Snowflake for production  
- **Parquet partitioning** â†’ Enables efficient historical queries  
- **Airflow standalone** â†’ Simple for demo; MWAA/K8s for production  
- **Streamlit** â†’ Fast prototyping; Superset/Tableau for enterprise BI  
- **Checkpoints clearing** â†’ Safe in dev to replay from earliest; avoid in prod to prevent duplicates  

---